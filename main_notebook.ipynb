{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from data_loader import get_loader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from processData import Vocabulary\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from scipy import misc\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
    "glove_vectors = pickle.load(open('glove.6B/glove_words.pkl', 'rb'))\n",
    "glove_vectors = torch.tensor(glove_vectors)\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "BertModel(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): BertLayerNorm()\n",
    "    (dropout): Dropout(p=0.1)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (1): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (2): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (3): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (4): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (5): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (6): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (7): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (8): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (9): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (10): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "      (11): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): BertLayerNorm()\n",
    "            (dropout): Dropout(p=0.1)\n",
    "          )\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): BertLayerNorm()\n",
    "          (dropout): Dropout(p=0.1)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (pooler): BertPooler(\n",
    "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "    (activation): Tanh()\n",
    "  )\n",
    ")\n",
    "Models\n",
    "#####################################\n",
    "# Encoder RASNET CNN - pretrained\n",
    "#####################################\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14, 14))\n",
    "\n",
    "    def forward(self, images):\n",
    "        out = self.adaptive_pool(self.resnet(images))\n",
    "        # batch_size, img size, imgs size, 2048\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "####################\n",
    "# Attention Decoder\n",
    "####################\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, use_glove, use_bert):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.encoder_dim = 2048\n",
    "        self.attention_dim = 512\n",
    "        self.use_bert = use_bert\n",
    "        if use_glove:\n",
    "            self.embed_dim = 300\n",
    "        elif use_bert:\n",
    "            self.embed_dim = 768\n",
    "        else:\n",
    "            self.embed_dim = 512\n",
    "            \n",
    "        self.decoder_dim = 512\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = 0.5\n",
    "        \n",
    "        # soft attention\n",
    "        self.enc_att = nn.Linear(2048, 512)\n",
    "        self.dec_att = nn.Linear(512, 512)\n",
    "        self.att = nn.Linear(512, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # decoder layers\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(self.embed_dim + self.encoder_dim, self.decoder_dim, bias=True)\n",
    "        self.h_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.c_lin = nn.Linear(self.encoder_dim, self.decoder_dim)\n",
    "        self.f_beta = nn.Linear(self.decoder_dim, self.encoder_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(self.decoder_dim, self.vocab_size)\n",
    "\n",
    "        # init variables\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        if not use_bert:\n",
    "            self.embedding = nn.Embedding(vocab_size, self.embed_dim)\n",
    "            self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "            # load Glove embeddings\n",
    "            if use_glove:\n",
    "                self.embedding.weight = nn.Parameter(glove_vectors)\n",
    "\n",
    "            # always fine-tune embeddings (even with GloVe)\n",
    "            for p in self.embedding.parameters():\n",
    "                p.requires_grad = True\n",
    "            \n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):    \n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        dec_len = [x-1 for x in caption_lengths]\n",
    "        max_dec_len = max(dec_len)\n",
    "\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        if not self.use_bert:\n",
    "            embeddings = self.embedding(encoded_captions)\n",
    "        elif self.use_bert:\n",
    "            embeddings = []\n",
    "            for cap_idx in  encoded_captions:\n",
    "                \n",
    "                # padd caption to correct size\n",
    "                while len(cap_idx) < max_dec_len:\n",
    "                    cap_idx.append(PAD)\n",
    "                    \n",
    "                cap = ' '.join([vocab.idx2word[word_idx.item()] for word_idx in cap_idx])\n",
    "                cap = u'[CLS] '+cap\n",
    "                \n",
    "                tokenized_cap = tokenizer.tokenize(cap)                \n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_cap)\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    encoded_layers, _ = model(tokens_tensor)\n",
    "\n",
    "                bert_embedding = encoded_layers[11].squeeze(0)\n",
    "                \n",
    "                split_cap = cap.split()\n",
    "                tokens_embedding = []\n",
    "                j = 0\n",
    "\n",
    "                for full_token in split_cap:\n",
    "                    curr_token = ''\n",
    "                    x = 0\n",
    "                    for i,_ in enumerate(tokenized_cap[1:]): # disregard CLS\n",
    "                        token = tokenized_cap[i+j]\n",
    "                        piece_embedding = bert_embedding[i+j]\n",
    "                        \n",
    "                        # full token\n",
    "                        if token == full_token and curr_token == '' :\n",
    "                            tokens_embedding.append(piece_embedding)\n",
    "                            j += 1\n",
    "                            break\n",
    "                        else: # partial token\n",
    "                            x += 1\n",
    "                            \n",
    "                            if curr_token == '':\n",
    "                                tokens_embedding.append(piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                            else:\n",
    "                                tokens_embedding[-1] = torch.add(tokens_embedding[-1], piece_embedding)\n",
    "                                curr_token += token.replace('#', '')\n",
    "                                \n",
    "                                if curr_token == full_token: # end of partial\n",
    "                                    j += x\n",
    "                                    break                            \n",
    "                \n",
    "                               \n",
    "                cap_embedding = torch.stack(tokens_embedding)\n",
    "\n",
    "                embeddings.append(cap_embedding)\n",
    "                \n",
    "            embeddings = torch.stack(embeddings)\n",
    "\n",
    "        # init hidden state\n",
    "        avg_enc_out = encoder_out.mean(dim=1)\n",
    "        h = self.h_lin(avg_enc_out)\n",
    "        c = self.c_lin(avg_enc_out)\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_dec_len, vocab_size)\n",
    "        alphas = torch.zeros(batch_size, max_dec_len, num_pixels)\n",
    "\n",
    "        for t in range(max(dec_len)):\n",
    "            batch_size_t = sum([l > t for l in dec_len ])\n",
    "            \n",
    "            # soft-attention\n",
    "            enc_att = self.enc_att(encoder_out[:batch_size_t])\n",
    "            dec_att = self.dec_att(h[:batch_size_t])\n",
    "            att = self.att(self.relu(enc_att + dec_att.unsqueeze(1))).squeeze(2)\n",
    "            alpha = self.softmax(att)\n",
    "            attention_weighted_encoding = (encoder_out[:batch_size_t] * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        \n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            batch_embeds = embeddings[:batch_size_t, t, :]  \n",
    "            cat_val = torch.cat([batch_embeds.double(), attention_weighted_encoding.double()], dim=1)\n",
    "            \n",
    "            h, c = self.decode_step(cat_val.float(),(h[:batch_size_t].float(), c[:batch_size_t].float()))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            \n",
    "        # preds, sorted capts, dec lens, attention wieghts\n",
    "        return predictions, encoded_captions, dec_len, alphas\n",
    "Model Setup\n",
    "# loss\n",
    "class loss_obj(object):\n",
    "    def __init__(self):\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0.\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "PAD = 0\n",
    "START = 1\n",
    "END = 2\n",
    "UNK = 3\n",
    "\n",
    "grad_clip = 5.\n",
    "num_epochs = 1\n",
    "batch_size = 32\n",
    "decoder_lr = 0.0004\n",
    "\n",
    "# Load vocabulary wrapper\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# load data\n",
    "train_loader = get_loader('train', vocab, batch_size)\n",
    "val_loader = get_loader('val', vocab, batch_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loading annotations into memory...\n",
    "Done (t=1.75s)\n",
    "creating index...\n",
    "index created!\n",
    "loading annotations into memory...\n",
    "Done (t=1.06s)\n",
    "creating index...\n",
    "index created!\n",
    "len(vocab)\n",
    "8856\n",
    "#############\n",
    "# New model\n",
    "#############\n",
    "\n",
    "# decoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True) # true = use glove\n",
    "# decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n",
    "\n",
    "# encoder = Encoder()\n",
    "\n",
    "#############\n",
    "# Load model\n",
    "#############\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder_checkpoint = torch.load('./checkpoints/encoder_baseline',map_location='cpu')\n",
    "encoder.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "decoder = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n",
    "decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr)\n",
    "decoder_checkpoint = torch.load('./checkpoints/decoder_baseline',map_location='cpu')\n",
    "decoder.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "Train\n",
    "def train():\n",
    "    print(\"Started training...\")\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        decoder.train()\n",
    "        encoder.train()\n",
    "\n",
    "        losses = loss_obj()\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for i, (imgs, caps, caplens) in enumerate(tqdm(train_loader)):\n",
    "            if i > 20:\n",
    "                break\n",
    "\n",
    "            imgs = encoder(imgs)\n",
    "\n",
    "            scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "            targets = caps_sorted[:, 1:]\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "            loss = criterion(scores, targets)\n",
    "            loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # grad_clip decoder\n",
    "            for group in decoder_optimizer.param_groups:\n",
    "                for param in group['params']:\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n",
    "\n",
    "            # save model each 100 batches\n",
    "            if i%60==0 and i!=0:\n",
    "                print('Batch '+str(i)+'/'+str(num_batches)+' loss:'+str(losses.avg))\n",
    "                \n",
    "                 # adjust learning rate (create condition for this)\n",
    "                for param_group in decoder_optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr'] * 0.5\n",
    "\n",
    "                print('saving model...')\n",
    "\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': decoder.state_dict(),\n",
    "                    'optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, './checkpoints/decoder_4')\n",
    "\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': encoder.state_dict(),\n",
    "                    'loss': loss,\n",
    "                    }, './checkpoints/encoder_4')\n",
    "\n",
    "                print('model saved')\n",
    "        \n",
    "        print('Epoch Loss:'+str(losses.avg))\n",
    "\n",
    "    print(\"Completed training...\")  \n",
    "Validate\n",
    "%matplotlib inline\n",
    "\n",
    "def print_sample(hypotheses,references,imgs, alphas, k, show_att):\n",
    "    print('Baseline Model')\n",
    "    img_dim = 500 # 14*24\n",
    "    \n",
    "    hyp_sentence = []\n",
    "    for word_idx in hypotheses[k]:\n",
    "        hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    ref_sentence = []\n",
    "    for word_idx in references[k]:\n",
    "        ref_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    img = imgs[0][k] \n",
    "    misc.imsave('img.jpg', img)\n",
    "  \n",
    "    if show_att:\n",
    "        image = Image.open('img.jpg')\n",
    "        image = image.resize([img_dim, img_dim], Image.LANCZOS)\n",
    "        for t in range(len(hyp_sentence)):\n",
    "\n",
    "            plt.subplot(np.ceil(len(hyp_sentence) / 5.), 5, t + 1)\n",
    "\n",
    "            plt.text(0, 1, '%s' % (hyp_sentence[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "            plt.imshow(image,cmap='gray')\n",
    "            current_alpha = alphas[0][t, :].detach().numpy()\n",
    "            alpha = skimage.transform.resize(current_alpha, [img_dim, img_dim])\n",
    "            if t == 0:\n",
    "                plt.imshow(alpha, alpha=0,cmap='gray')\n",
    "            else:\n",
    "                plt.imshow(alpha, alpha=0.7,cmap='gray')\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        img = misc.imread('img.jpg')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "    print('Hypotheses: '+\" \".join(hyp_sentence))\n",
    "    print('References: '+\" \".join(ref_sentence))\n",
    "references = [] \n",
    "test_references = []\n",
    "hypotheses = [] \n",
    "all_imgs = []\n",
    "all_alphas = []\n",
    "\n",
    "def validate():\n",
    "    print(\"Started validation...\")\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "\n",
    "    losses = loss_obj()\n",
    "\n",
    "    num_batches = len(val_loader)\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        scores_packed = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "        targets_packed = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores_packed, targets_packed)\n",
    "        loss += ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            test_references.append(clean_cap)\n",
    "            references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds = torch.max(scores, dim=2)\n",
    "        preds = preds.tolist()\n",
    "        temp_preds = list()\n",
    "        for j, p in enumerate(preds):\n",
    "            pred = p[:decode_lengths[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds.append(pred)  # remove pads, start, and end\n",
    "        preds = temp_preds\n",
    "        hypotheses.extend(preds)\n",
    "        \n",
    "        all_alphas.append(alphas)\n",
    "        all_imgs.append(imgs_jpg)\n",
    "\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
    "    bleu_2 = corpus_bleu(references, hypotheses, weights=(0, 1, 0, 0))\n",
    "    bleu_3 = corpus_bleu(references, hypotheses, weights=(0, 0, 1, 0))\n",
    "    bleu_4 = corpus_bleu(references, hypotheses, weights=(0, 0, 0, 1))\n",
    "\n",
    "    print(\"Validation loss: \"+str(losses.avg))\n",
    "    print(\"BLEU: \"+str(bleu))\n",
    "    print(\"BLEU-1: \"+str(bleu_1))\n",
    "    print(\"BLEU-2: \"+str(bleu_2))\n",
    "    print(\"BLEU-3: \"+str(bleu_3))\n",
    "    print(\"BLEU-4: \"+str(bleu_4))\n",
    "    print_sample(hypotheses, test_references, all_imgs, all_alphas,1,False)\n",
    "    print(\"Completed validation...\")\n",
    "%matplotlib inline\n",
    "\n",
    "def compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, imgs, k):\n",
    "    img_dim = 500 # 14*24\n",
    "    \n",
    "    bert_hyp_sentence = []\n",
    "    for word_idx in bert_hypotheses[k]:\n",
    "        bert_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    glove_hyp_sentence = []\n",
    "    for word_idx in glove_hypotheses[k]:\n",
    "        glove_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    baseline_hyp_sentence = []\n",
    "    for word_idx in baseline_hypotheses[k]:\n",
    "        baseline_hyp_sentence.append(vocab.idx2word[word_idx])\n",
    "    \n",
    "    ref_sentence = []\n",
    "    for word_idx in references[k]:\n",
    "        ref_sentence.append(vocab.idx2word[word_idx])\n",
    "        \n",
    "    img = imgs[0][k] \n",
    "    misc.imsave('img.jpg', img)\n",
    "  \n",
    "    img = misc.imread('img.jpg')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "    print('References: '+\" \".join(ref_sentence))\n",
    "    print(' ')\n",
    "    print('Baseline  : '+\" \".join(baseline_hyp_sentence))\n",
    "    print('GloVe     : '+\" \".join(glove_hyp_sentence))\n",
    "    print('BERT      : '+\" \".join(bert_hyp_sentence))\n",
    "    \n",
    "references = [] \n",
    "bert_hypotheses = []\n",
    "glove_hypotheses = [] \n",
    "baseline_hypotheses = [] \n",
    "all_imgs = []\n",
    "\n",
    "def compare_models():\n",
    "    \n",
    "    # load all pre-trained models\n",
    "    \n",
    "    encoder_glove = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints/encoder_glove',map_location='cpu')\n",
    "    encoder_glove.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_glove = Decoder(vocab_size=len(vocab),use_glove=True, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_glove.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints/decoder_glove',map_location='cpu')\n",
    "    decoder_glove.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_bert = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints/encoder_bert',map_location='cpu')\n",
    "    encoder_bert.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_bert = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=True)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_bert.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints/decoder_bert',map_location='cpu')\n",
    "    decoder_bert.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    #########\n",
    "\n",
    "    encoder_baseline = Encoder()\n",
    "    encoder_checkpoint = torch.load('./checkpoints/encoder_baseline',map_location='cpu')\n",
    "    encoder_baseline.load_state_dict(encoder_checkpoint['model_state_dict'])\n",
    "    decoder_baseline = Decoder(vocab_size=len(vocab),use_glove=False, use_bert=False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder_baseline.parameters(),lr=decoder_lr)\n",
    "    decoder_checkpoint = torch.load('./checkpoints/decoder_baseline',map_location='cpu')\n",
    "    decoder_baseline.load_state_dict(decoder_checkpoint['model_state_dict'])\n",
    "    decoder_optimizer.load_state_dict(decoder_checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(\"Started Comparison...\")\n",
    "    decoder_bert.eval()\n",
    "    encoder_bert.eval()\n",
    "    decoder_glove.eval()\n",
    "    decoder_baseline.eval()\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(tqdm(val_loader)):\n",
    "        if i > 0:\n",
    "            break\n",
    "\n",
    "        imgs_jpg = imgs.numpy() \n",
    "        imgs_jpg = np.swapaxes(np.swapaxes(imgs_jpg, 1, 3), 1, 2)\n",
    "        \n",
    "        # Forward prop.\n",
    "        imgs = encoder_bert(imgs)\n",
    "        scores_bert, caps_sorted_bert,decode_lengths_bert , _ = decoder_bert(imgs, caps, caplens)\n",
    "        targets = caps_sorted_bert[:, 1:]\n",
    "        scores_glove, caps_sorted_glove, decode_lengths_glove, _ = decoder_glove(imgs, caps, caplens)\n",
    "        scores_baseline, caps_sorted_baseline, decode_lengths_baseline, _ = decoder_baseline(imgs, caps, caplens)\n",
    "\n",
    "         # References\n",
    "        for j in range(targets.shape[0]):\n",
    "            img_caps = targets[j].tolist() # validation dataset only has 1 unique caption per img\n",
    "            clean_cap = [w for w in img_caps if w not in [PAD, START, END]]  # remove pad, start, and end\n",
    "            img_captions = list(map(lambda c: clean_cap,img_caps))\n",
    "            references.append(clean_cap)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds_bert = torch.max(scores_bert, dim=2)\n",
    "        _, preds_glove = torch.max(scores_glove, dim=2)\n",
    "        _, preds_baseline = torch.max(scores_baseline, dim=2)\n",
    "        preds_bert = preds_bert.tolist()\n",
    "        preds_glove = preds_glove.tolist()\n",
    "        preds_baseline = preds_baseline.tolist()\n",
    "        \n",
    "        temp_preds_bert = list()\n",
    "        temp_preds_glove = list()\n",
    "        temp_preds_baseline = list()\n",
    "        \n",
    "        for j, p in enumerate(preds_bert):\n",
    "            pred = preds_bert[j][:decode_lengths_bert[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_bert.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_glove[j][:decode_lengths_glove[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_glove.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "            pred = preds_baseline[j][:decode_lengths_baseline[j]]\n",
    "            pred = [w for w in pred if w not in [PAD, START, END]]\n",
    "            temp_preds_baseline.append(pred)  # remove pads, start, and end\n",
    "            \n",
    "        bert_hypotheses.extend(temp_preds_bert)\n",
    "        glove_hypotheses.extend(temp_preds_glove)\n",
    "        baseline_hypotheses.extend(temp_preds_baseline)\n",
    "        all_imgs.append(imgs_jpg)\n",
    "        \n",
    "        compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, all_imgs, 1)\n",
    "Run Train/Test\n",
    "# compare_models()\n",
    "# compare_sample(references,bert_hypotheses,glove_hypotheses, baseline_hypotheses, all_imgs, 12)\n",
    "# train()\n",
    "validate()\n",
    "Started validation...\n",
    "Validation loss: 3.26445436478\n",
    "BLEU: 0.227002810443\n",
    "BLEU-1: 0.532142857143\n",
    "BLEU-2: 0.225\n",
    "BLEU-3: 0.175572519084\n",
    "BLEU-4: 0.126315789474\n",
    "Baseline Model\n",
    "\n",
    "Hypotheses: a plate plate topped a on on on a . a fork of food . it .\n",
    "References: a white plate with cowboy logos engraved on them with a couple of brownies on it .\n",
    "Completed validation...\n",
    "print_sample(hypotheses, test_references,all_imgs,all_alphas,10, False)\n",
    "Baseline Model\n",
    "\n",
    "Hypotheses: a man riding a surfboard on top of a surfboard surfboard .\n",
    "References: a man riding a wave on top of a yellow surfboard .\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
